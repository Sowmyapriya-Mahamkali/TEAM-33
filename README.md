# TEAM-33 â€” GenAI-Powered Real-Time Multilingual AI Agent

ðŸŽ¥ **Project Demo Video**  
ðŸ‘‰ https://drive.google.com/file/d/1Bu0zHV2MonN1LOHjehUz5SoCTc8_aSE3/view?usp=sharing


## Problem Statement

Effective communication in multilingual environmentsâ€”such as healthcare, education, and global workplacesâ€”remains a challenge. Existing translation tools are often delayed, screen-dependent, or lack contextual understanding, resulting in miscommunication, inefficiency, and cognitive overload.

There is a critical need for an **intelligent, real-time system** capable of understanding spoken language, maintaining context, and generating accurate multilingual responses seamlessly.


## Proposed Solution

We present a **GenAI-powered real-time multilingual AI agent** designed to facilitate seamless bidirectional communication between users speaking different languages.

The system leverages **foundation models for speech recognition and language generation**, enhanced with **Retrieval-Augmented Generation (RAG)** for context-aware accuracy.

**Workflow Overview:**

1. **Listen** â€” AI captures live speech and converts it to text using advanced speech recognition models (Whisper).  
2. **Understand & Retrieve** â€” The AI Agent identifies speaker language, maintains conversation context, and retrieves relevant knowledge for accurate responses.  
3. **Generate & Respond** â€” The AI generates precise multilingual translations in real time, enabling natural conversation flow.  

> *Note: While the future vision includes integration with smart earbuds for hands-free delivery, the current prototype focuses entirely on the AI-driven real-time translation system.*>

## Key Highlights

- **Autonomous AI Agent:** Performs real-time decision-making for language detection, context maintenance, and response generation.  
- **Generative AI (GenAI) Integration:** Produces natural, context-aware multilingual responses.  
- **RAG-Enhanced Accuracy:** Ensures translations respect domain-specific vocabulary and conversational history.  
- **Scalable & Extensible:** Designed for integration with future hardware interfaces, such as smart earbuds.  


**Structure**
```
*TEAM-33*/
â”‚
â”œâ”€â”€ ai_agent/                 # Core AI Agent modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ asr.py                # Whisper speech-to-text module
â”‚   â”œâ”€â”€ translation.py        # LLM translation module
â”‚   â”œâ”€â”€ rag.py                # Retrieval-Augmented Generation logic
â”‚   â”œâ”€â”€ tts.py                # Text-to-speech module
â”‚   â””â”€â”€ agent_controller.py   # AI agent orchestration
â”‚
â”œâ”€â”€ backend/                  # Backend server (FastAPI / Flask)
â”‚   â”œâ”€â”€ main.py               # API entrypoint
â”‚   â”œâ”€â”€ routes.py             # API routes
â”‚   â””â”€â”€ requirements.txt      # Dependencies
â”‚
â”œâ”€â”€ frontend/                 # Optional simulated UI for hackathon
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ app.js
â”‚   â””â”€â”€ style.css
â”‚
â”œâ”€â”€ tests/                    # Unit / integration tests
â”‚   â”œâ”€â”€ test_asr.py
â”‚   â”œâ”€â”€ test_translation.py
â”‚   â””â”€â”€ test_rag.py
â”‚
â”œâ”€â”€ data/                     # Sample input/output data (optional)
â”‚   â”œâ”€â”€ sample_audio/
â”‚   â””â”€â”€ sample_translations/
â”‚
â”œâ”€â”€ README.md                 # Problem statement + solution + project structure
â”œâ”€â”€ requirements.txt          # All Python dependencies
â””â”€â”€ .gitignore                # Ignore virtual env, logs, etc.
```
## Architecture Overview

The system is designed as a modular, agent-oriented pipeline that processes multilingual conversations end-to-end in real time. Each component is responsible for a well-defined function and can be independently scaled or replaced.

**End-to-end pipeline:**

Speech Input â†’ ASR â†’ Context & RAG â†’ LLM Reasoning â†’ TTS â†’ Speech Output

The AI Agent Controller coordinates all modules, manages conversation state, and ensures low-latency execution.

---

## Core Components

### Automatic Speech Recognition (ASR)
- Converts live or recorded speech into text
- Automatically detects the source language
- Built using Whisper for robustness to accents and noise

### Retrieval-Augmented Generation (RAG)
- Retrieves domain-specific and conversational context
- Grounds LLM responses to reduce hallucinations
- Supports local knowledge base with vector-database extensibility

### Generative AI (LLM Layer)
- Generates context-aware multilingual responses
- Maintains conversational history
- Supports configurable foundation models (e.g., GPT-4, Claude)

### Text-to-Speech (TTS)
- Converts generated responses into natural speech
- Supports multiple languages and neural voices
- Designed for real-time voice delivery

### Agent Controller
- Orchestrates ASR, RAG, LLM, and TTS modules
- Manages session context and error handling
- Provides a unified interface for file-based and live audio processing

---

## Technology Stack

- Speech Recognition: OpenAI Whisper  
- Language Models: GPT-4 / Claude (configurable)  
- Context Retrieval: Retrieval-Augmented Generation (RAG)  
- Speech Synthesis: Azure Speech Services / Google Cloud TTS  
- Backend: Python (FastAPI / Flask compatible)  
- Frontend: HTML, CSS, JavaScript (demo interface)  

---

## Design Principles

- Voice-First Interaction  
  Enables natural communication without reliance on screens.

- Context Preservation  
  Maintains conversational and domain context across interactions.

- Modular Architecture  
  Allows independent development, testing, and scaling of components.

- Extensibility  
  Designed to integrate with hardware interfaces such as smart earbuds and telephony systems.

---

## Application Scenarios

- Healthcare:  
  Real-time multilingual doctorâ€“patient communication with preserved context.

- Education:  
  Multilingual classrooms and remote learning environments.

- Enterprise & Global Teams:  
  Seamless cross-language collaboration.

- Customer Support:  
  Voice-based multilingual assistance without human interpreters.

---

## Testing and Validation

The project includes unit and integration tests to validate:
- Speech-to-text accuracy
- Context retrieval correctness
- Consistency of multilingual responses

This ensures reliability and simplifies future enhancements.

---

## Limitations

- Dependent on network connectivity for cloud-based models  
- End-to-end latency varies based on ASR and LLM response time  
- Offline functionality is limited in the current prototype  

---

## Future Roadmap

- Smart earbud integration for hands-free operation  
- Offline speech recognition for low-connectivity environments  
- Emergency detection and alerting mechanisms  
- Integration with electronic health record (EHR) systems  
- Expansion to additional domains beyond healthcare  

---

## Summary

This project demonstrates a production-oriented GenAI multilingual AI agent that combines real-time speech processing, contextual retrieval, and generative reasoning.
The architecture is designed for accuracy, scalability, and real-world deployment, making it suitable for high-impact multilingual environments.
